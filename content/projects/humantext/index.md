---
title: "Humans Generate Text Too - Mistakes in GPT and Me."
date: "2023-04-08"
draft: true
socialShare: true
math: true
---

{{< warning Disclaimer>}}
These are ponderings... While I have Neuroscience research degrees, and have done it professionally for a long time, my field of expertise is not human language.
{{< /warning >}}

## Working through the FastAi course

While working throught the fastAi course, this quote about text generation caught my eye:

> "It's good at making this content compelling to humans tooâ€”in fact, even more compelling than human-generated text. However, deep learning is currently not good at generating correct responses!"

I also know this from personal experience, I've had chatGPT hallucinate made up academic references before, they looked convincing, but they weren't real. This reminded me of something humans do a lot - **confabulation**. For me, this raised the following question:

> **What if *correct* text generation is a fundamental problem for text generation, in both humans and in AI?**

I want to discuss a few pieces of neuroscience and psychology research I've read, and few pieces from my own field to help remind myself of something...

> **I am a text generator too.**

When I tell stories, like my identity, goals, beliefs, politics, feelings, and causes for those feelings, that doesn't meen they're true. That doesn't mean they won't be compelling either.

## What do I mean by fundamental problem?

Psychometric example. It doesn't matter if the mechanism is completely different.

## Confabulation

## We have a long history of being wrong and convincing

## A lot about generating text isn't about what's real

Being careful when interpreting text - therapy, how do you feel?

## Conclusions

The philosopohical problem underneath. Subjectivity.

My conclusion: If you go to words to find the answer be careful.

Code block:
  
  ```shell
  hugo server --buildDrafts
  ```